{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Machine Learning\n","The code below creates a neural network and fits it to the training data. I used this code as a second model for comparison. It is discussed in section 2.2.2 in the pdf report, but is not the model discussed in the majority of the report. Before runing, make sure that the covariates data file is saved as \"data/tcga.csv\"."]},{"cell_type":"markdown","metadata":{},"source":["### Imports"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from scikeras.wrappers import KerasRegressor\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"markdown","metadata":{},"source":["### Global vaiables"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["CWD = os.path.abspath(os.path.join(os.path.abspath(''), \"..\"))\n","ROOT_DIR = os.path.join(CWD, \"../\")\n","DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Model definition fuction\n","\n","This is the function, that when called, defines the model. I have commented out the second hidden layer here, so it can easily be re-instated when testing the two hidden layer architecture. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def baseline_model(input_dim, hidden_dim1, hidden_dim2):\n","    # create model\n","    model = Sequential()\n","    model.add(\n","        Dense(\n","            hidden_dim1,\n","            input_dim=input_dim,\n","            kernel_initializer=\"normal\",\n","            activation=\"relu\",\n","        )\n","    )\n","    # model.add(\n","    #     Dense(\n","    #         hidden_dim2,\n","    #         kernel_initializer=\"normal\",\n","    #         activation=\"relu\",\n","    #     )\n","    # )\n","    model.add(Dense(1, kernel_initializer=\"normal\"))\n","    # Compile model\n","    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n","    return model\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Fix the random seed to stop unecessary random variation between runs."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["seed = 7\n","np.random.seed(seed)"]},{"cell_type":"markdown","metadata":{},"source":["### Loads\n","Here I load the preprocessed file as generated in the random forest notebook."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preprocessed_outfile = os.path.join(DATA_DIR, \"tcga_preprocessed.csv\")\n","df = pd.read_csv(os.path.join(DATA_DIR, preprocessed_outfile))\n","feature_columns = [c for c in df.columns if \"gene_\" in c or c == \"treatment\"]\n","feature_df = df[feature_columns]\n","outcome = df[\"outcome\"]"]},{"cell_type":"markdown","metadata":{},"source":["### Defining the Model\n","Here I define the number of nodes in each layer and instantiate the model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["input_dim = 4001\n","hidden_dim1 = input_dim\n","hidden_dim2 = hidden_dim1 // 2\n","\n","model = KerasRegressor(model=baseline_model(input_dim, hidden_dim1, hidden_dim2))"]},{"cell_type":"markdown","metadata":{},"source":["### Hyper-parameter optimisation\n","Define the parameters for the cross-validated grid search "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use scikit-learn to grid search the batch size and epochs\n","# define the grid search parameters\n","\n","params = {\n","    \"batch_size\": [60],\n","    \"epochs\": [\n","        60,\n","        70,\n","        80,\n","        90,\n","        100,\n","    ],\n","}\n","CV_regressor = GridSearchCV(\n","    estimator=model,\n","    param_grid=params,\n","    # n_jobs=-1, # Uncomment this line to use all available processors\n","    cv=3,\n","    verbose=2,\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["Define the test/train split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(\n","    feature_df, outcome, test_size=0.25, random_state=5\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Fit the cross-validated regressor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CV_regressor.fit(X_train, y_train)\n","\n","y_pred = CV_regressor.predict(X_test)\n","y_pred_train = CV_regressor.predict(X_train)"]},{"cell_type":"markdown","metadata":{},"source":["### Output the results of the grid search to file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cv_results_file = \"output/neural_net_gridsearch_results5.csv\"\n","cv_results = pd.DataFrame(data=CV_regressor.cv_results_)\n","cv_outfile = os.path.join(ROOT_DIR, cv_results_file)\n","cv_results.to_csv(cv_outfile, index=False)\n","display(pd.DataFrame(data=CV_regressor.cv_results_))"]},{"cell_type":"markdown","metadata":{},"source":["### Create plots to show the relative performances of the different hyper-parameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cv_results_file = os.path.join(ROOT_DIR, cv_results_file)\n","cv_results = pd.read_csv(cv_results_file)\n","for idx, group in cv_results.groupby(by=[\"param_batch_size\"]):\n","    fig, ax = plt.subplots()\n","    epochs = [\"10\", \"50\", \"100\"]\n","    x_pos = range(len(epochs))\n","    mean_10 = group.loc[group[\"param_epochs\"] == 10][\"mean_test_score\"].values[0]\n","    mean_50 = group.loc[group[\"param_epochs\"] == 50][\"mean_test_score\"].values[0]\n","    mean_100 = group.loc[group[\"param_epochs\"] == 100][\"mean_test_score\"].values[0]\n","    std_10 = group.loc[group[\"param_epochs\"] == 10][\"std_test_score\"].values[0]\n","    std_50 = group.loc[group[\"param_epochs\"] == 50][\"std_test_score\"].values[0]\n","    std_100 = group.loc[group[\"param_epochs\"] == 100][\"std_test_score\"].values[0]\n","\n","    test_scores = [mean_10, mean_50, mean_100]\n","    error = [std_10, std_50, std_100]\n","    ax.bar(\n","        x_pos,\n","        test_scores,\n","        yerr=error,\n","        align=\"center\",\n","        alpha=0.5,\n","        ecolor=\"black\",\n","        capsize=10,\n","    )\n","    ax.set_ylabel(\"Test score\")\n","    ax.set_xlabel(\"epochs\")\n","    ax.set_xticks(x_pos)\n","    ax.set_xticklabels(epochs)\n","    ax.set_title(f\"epochs comparison - param_batch_size={idx}\")\n","    ax.yaxis.grid(True)\n","\n","    # Save the figure and show\n","    plt.tight_layout()\n","    epochs_comparison_plot_file = os.path.join(\n","        ROOT_DIR,\n","        f\"output/machine_learning_plots/epochs_comparison_param_batch_size={idx}.png\",\n","    )\n","    plt.savefig(epochs_comparison_plot_file, bbox_inches=\"tight\")\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cv_results_file = \"output/neural_net_gridsearch_results2.csv\"\n","cv_results_file = os.path.join(ROOT_DIR, cv_results_file)\n","cv_results = pd.read_csv(cv_results_file)\n","for idx, group in cv_results.groupby(by=[\"param_epochs\"]):\n","    fig, ax = plt.subplots()\n","    batch_sizes = [\"10\", \"20\", \"40\", \"60\", \"80\", \"100\"]\n","    x_pos = range(len(batch_sizes))\n","    mean_10 = group.loc[group[\"param_batch_size\"] == 10][\"mean_test_score\"].values[0]\n","    mean_20 = group.loc[group[\"param_batch_size\"] == 20][\"mean_test_score\"].values[0]\n","    mean_40 = group.loc[group[\"param_batch_size\"] == 40][\"mean_test_score\"].values[0]\n","    mean_60 = group.loc[group[\"param_batch_size\"] == 60][\"mean_test_score\"].values[0]\n","    mean_80 = group.loc[group[\"param_batch_size\"] == 80][\"mean_test_score\"].values[0]\n","    mean_100 = group.loc[group[\"param_batch_size\"] == 100][\"mean_test_score\"].values[0]\n","    std_10 = group.loc[group[\"param_batch_size\"] == 10][\"std_test_score\"].values[0]\n","    std_20 = group.loc[group[\"param_batch_size\"] == 20][\"std_test_score\"].values[0]\n","    std_40 = group.loc[group[\"param_batch_size\"] == 40][\"std_test_score\"].values[0]\n","    std_60 = group.loc[group[\"param_batch_size\"] == 60][\"std_test_score\"].values[0]\n","    std_80 = group.loc[group[\"param_batch_size\"] == 80][\"std_test_score\"].values[0]\n","    std_100 = group.loc[group[\"param_batch_size\"] == 100][\"std_test_score\"].values[0]\n","\n","    test_scores = [mean_10, mean_20, mean_40, mean_60, mean_80, mean_100]\n","    error = [std_10, std_20, std_40, std_60, std_80, std_100]\n","    ax.bar(\n","        x_pos,\n","        test_scores,\n","        yerr=error,\n","        align=\"center\",\n","        alpha=0.5,\n","        ecolor=\"black\",\n","        capsize=10,\n","    )\n","    ax.set_ylabel(\"Test score\")\n","    ax.set_xlabel(\"batch_size\")\n","    ax.set_xticks(x_pos)\n","    ax.set_xticklabels(batch_sizes)\n","    ax.set_title(f\"epochs comparison - param_epoch={idx}\")\n","    ax.yaxis.grid(True)\n","\n","    # Save the figure and show\n","    plt.tight_layout()\n","    batch_size_comparison_plot_file = os.path.join(\n","        ROOT_DIR,\n","        f\"output/machine_learning_plots/batch_size_comparison_param_epoch={idx}.png\",\n","    )\n","    plt.savefig(batch_size_comparison_plot_file, bbox_inches=\"tight\")\n","    plt.show()\n"]}],"metadata":{"interpreter":{"hash":"ad8d43620b3925f08ce110e9037cdfdf522485435230516688e36481e087fe75"},"kernelspec":{"display_name":"Python 3.9.7 ('env': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
